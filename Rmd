---
title: "Project"
author: "MICHAEL AHANA"
date: "2024-04-05"
output:
  pdf_document:
    latex_engine: xelatex
---

# 1

This involves the use of cross-validation in classification on the German Credit Risk. The purpose of this analysis is to implement a machine learning algorithm to predict the credit risk (good or bad) of a consumer in the German market. 

## Part A

The goal of this data analysis is to develop a machine learning model that accurately predicts the credit risk of consumers in the German Market. The aim is to predict whether a consumer's credit risk is categorized as "good" or "bad" based on feature available in the dataset.


   Inputs (Features):

a. Account Balance
b. Duration of Credit (month)
c. Payment Status of Previous Credit
d. Purpose
e. Credit Amount
f. Value Savings/Stocks
g. Length of current employment
h. Instalment per cent
i. Sex & Marital Status
j. Guarantors
k. Duration in Current address
l. Most valuable available asset
m. Age (years)
n. Concurrent Credits
o. Type of apartment
p. No of Credits at this Bank
q. Occupation
r. No of dependents
s. Telephone
t. Foreign Worker

  Output (Target):

u. Creditability

## EXPLORATORY ANALYSIS

Understanding the characteristics of the dataset before diving into model building or analysis.

### Structure of the data (Summary Statistics)
```{r}
# Read the CSV file into a data frame
credit_data <- read.csv("german_credit.csv")

# Check the structure of the data
str(credit_data)

# Display the first few rows of the data
head(credit_data)

```
### Checking for missing values 

```{r}
# Check for missing values in each column
missing_values <- colSums(is.na(credit_data))

# Display columns with missing values and their corresponding counts
missing_values[missing_values > 0]

```
it means that there are no missing values in the dataset. This is good news !

### Data Transformation

Feature Scaling: Feature scaling is important to ensure that features with different scales and units contribute equally to the model training process. In the dataset, some features like "Credit Amount" and "Age (years)" have much larger scales compared to others like "Duration of Credit (month)" and "Instalment per cent". Scaling these features can improve the performance of the machine learning algorithms.
```{r}
# Identify numerical columns excluding the response variable
numeric_columns <- names(credit_data)[sapply(credit_data, is.numeric)]
numeric_columns <- setdiff(numeric_columns, "Creditability")

# Standardize numerical features
credit_data[numeric_columns] <- scale(credit_data[numeric_columns])
head(credit_data)

```
This transformation ensures that features with larger scales do not dominate the learning process and helps the algorithm converge faster.



## Part B

Building a classifier to predict the creditability of a consumer using an appropriate machine
learning algorithm.
```{r}
# Load required library
library(caret)

# Define formula for logistic regression
formula <- Creditability ~ .

# Train logistic regression model
logistic_model <- glm(formula, data = credit_data, family = binomial)

# Summarize the model
summary(logistic_model)

```

### Variable Selection
 we use the step() function to perform stepwise selection. The argument direction = "both" allows the algorithm to consider both forward and backward steps.The final model will contain only the significant variables according to the chosen criteria 

Finally, display the summary of the final model to see which variables were selected and their coefficients.
```{r}
# Fit the initial model
initial_model <- glm(Creditability ~ ., family = binomial, data = credit_data)

# Perform stepwise selection
final_model <- step(initial_model, direction = "both")

# Summary of the final model
summary(final_model)

```

```{r}
# Extract the names of selected predictors from the final model
selected_predictors <- names(final_model$coefficients)[-1]  # Exclude intercept

# Fit logistic regression model using selected predictors
final_logit_model <- glm(Creditability ~ ., 
                         family = binomial, 
                         data = credit_data[, c("Creditability", selected_predictors)])

# Summary of the final logistic regression model
summary(final_logit_model)

```
The difference between the two models lies in the selection of predictors. In the first model, I used all available predictors, while in the second model, I used only the predictors selected through stepwise variable selection.

Stepwise variable selection iteratively adds or removes predictors based on their significance (p-values). Only predictors deemed statistically significant are retained in the final model.By selecting only the significant predictors, you focus the model's attention on the most informative features, potentially improving its performance in predicting the response variable.

## Algorithm performance

Print out your algorithm performance.
```{r}
library(pROC)

# Predict probabilities for each observation
predicted_probabilities <- predict(final_logit_model, type = "response")

# Convert probabilities to predicted classes (0 or 1) based on a threshold (e.g., 0.5)
predicted_classes <- ifelse(predicted_probabilities > 0.5, 1, 0)

# Confusion matrix
confusion_matrix <- table(Actual = credit_data$Creditability, Predicted = predicted_classes)
print("Confusion Matrix:")
print(confusion_matrix)

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", accuracy))

# Precision
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
print(paste("Precision:", precision))

# Recall (Sensitivity)
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
print(paste("Recall (Sensitivity):", recall))

# F1 Score
f1_score <- 2 * precision * recall / (precision + recall)
print(paste("F1 Score:", f1_score))

# ROC curve and AUC
roc <- roc(credit_data$Creditability, predicted_probabilities)

```
#### Result interpretation

The confusion matrix shows the count of true negative, false negative, true positive and false positive, in my case: 

TN: 141 (Predicted as not creditworthy and actually not creditworthy)
FP: 159 (Predicted as creditworthy but actually not creditworthy)
FN: 73 (Predicted as not creditworthy but actually creditworthy)
TP: 627 (Predicted as creditworthy and actually creditworthy)
This matrix provides a detailed view of the model's performance in terms of correct and incorrect predictions.

Accuracy: The accuracy of 0.768 indicates that the model correctly predicts the credit risk for approximately 76.8% of the observations in the dataset.However, accuracy alone may not be sufficient to evaluate the model's performance, especially if the classes are imbalanced.

Precision: The precision of 0.798 indicates that when the model predicts an individual as creditworthy, approximately 79.8% of the time, the individual is indeed creditworthy.

Recall: The recall of 0.896 indicates that the model correctly identifies approximately 89.6% of the creditworthy individuals in the dataset.


#### Iterate and improve algorithm performance

To implement k-fold cross-validation (CV) for logistic regression, you can follow these steps:

Split the dataset into k equal-sized folds.
For each fold:
a. Use k-1 folds as the training set and the remaining fold as the validation set.
b. Train the logistic regression model on the training set.
c. Evaluate the model's performance on the validation set.
Repeat steps 2a-2c for each fold.
Calculate the average performance metrics across all folds.

```{r}
library(caret)

# Define a function for k-fold cross-validation
logistic_regression_cv <- function(data, formula, k = 5) {
  # Initialize vectors to store performance metrics
  accuracy <- numeric(k)
  precision <- numeric(k)
  recall <- numeric(k)
  f1_score <- numeric(k)
  auc <- numeric(k)
  
  # Define the indices for k-fold cross-validation
  folds <- createFolds(data$Creditability, k = k, list = TRUE, returnTrain = FALSE)
  
  # Perform k-fold cross-validation
  for (i in 1:k) {
    # Split data into training and validation sets
    train_data <- data[-folds[[i]], ]
    validation_data <- data[folds[[i]], ]
    
    # Train logistic regression model
    model <- glm(formula, family = binomial, data = train_data)
    
    # Predict probabilities on validation set
    predicted_probabilities <- predict(model, newdata = validation_data, type = "response")
    predicted_classes <- ifelse(predicted_probabilities > 0.5, 1, 0)
    
    # Calculate performance metrics
    confusion_matrix <- table(Actual = validation_data$Creditability, Predicted = predicted_classes)
    accuracy[i] <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
    precision[i] <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
    recall[i] <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
    f1_score[i] <- 2 * precision[i] * recall[i] / (precision[i] + recall[i])
    
    # Calculate ROC curve and AUC
    roc_data <- roc(validation_data$Creditability, predicted_probabilities)
    auc[i] <- auc(roc_data)
  }
  
  # Calculate average performance metrics
  avg_accuracy <- mean(accuracy)
  avg_precision <- mean(precision)
  avg_recall <- mean(recall)
  avg_f1_score <- mean(f1_score)
  avg_auc <- mean(auc)
  
  # Print average performance metrics
  cat("Average Accuracy:", avg_accuracy, "\n")
  cat("Average Precision:", avg_precision, "\n")
  cat("Average Recall (Sensitivity):", avg_recall, "\n")
  cat("Average F1 Score:", avg_f1_score, "\n")
  cat("Average AUC:", avg_auc, "\n")
}

# Usage example:
# logistic_regression_cv(credit_data, Creditability ~ .)

```

```{r}
# Call the logistic_regression_cv function with your dataset and logistic regression formula
logistic_regression_cv(credit_data, Creditability ~ .)

```
#### Result interpretation

Average Accuracy: The proportion of correctly classified instances across all folds.
Average Precision: The average precision across all folds, focusing on the accuracy of positive predictions.
Average Recall (Sensitivity): The average recall across all folds, indicating the model's ability to correctly identify positive instances.
Average F1 Score: The harmonic mean of precision and recall, providing a balanced measure of model performance.
Average AUC: The average area under the ROC curve across all folds, representing the model's ability to discriminate between positive and negative instances.

#### Leave one out cross validation(LOOCV)

```{r}
library(caret)

# Define a function for leave-one-out cross-validation (LOOCV)
logistic_regression_loocv <- function(data, formula) {
  # Initialize vectors to store performance metrics
  accuracy <- numeric(nrow(data))
  precision <- numeric(nrow(data))
  recall <- numeric(nrow(data))
  f1_score <- numeric(nrow(data))
  auc <- numeric(nrow(data))
  
  # Perform leave-one-out cross-validation
  for (i in 1:nrow(data)) {
    # Split data into training and validation sets
    train_data <- data[-i, ]
    validation_data <- data[i, ]
    
    # Train logistic regression model
    model <- glm(formula, family = binomial, data = train_data)
    
    # Predict probability on the validation set
    predicted_probability <- predict(model, newdata = validation_data, type = "response")
    predicted_class <- ifelse(predicted_probability > 0.5, 1, 0)
    
    # Calculate performance metrics
    confusion_matrix <- table(Actual = validation_data$Creditability, Predicted = predicted_class)
    accuracy[i] <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
    precision[i] <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
    recall[i] <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
    f1_score[i] <- 2 * precision[i] * recall[i] / (precision[i] + recall[i])
    
    # Calculate ROC curve and AUC
    roc_data <- roc(validation_data$Creditability, predicted_probability)
    auc[i] <- auc(roc_data)
  }
  
  # Calculate average performance metrics
  avg_accuracy <- mean(accuracy)
  avg_precision <- mean(precision)
  avg_recall <- mean(recall)
  avg_f1_score <- mean(f1_score)
  avg_auc <- mean(auc)
  
  # Print average performance metrics
  cat("Average Accuracy:", avg_accuracy, "\n")
  cat("Average Precision:", avg_precision, "\n")
  cat("Average Recall (Sensitivity):", avg_recall, "\n")
  cat("Average F1 Score:", avg_f1_score, "\n")
  cat("Average AUC:", avg_auc, "\n")
}

# Usage example:
# logistic_regression_loocv(credit_data, Creditability ~ .)

```


# 3

Consider a classification problem with a large number of inputs, as may arise, for example, in genomic or proteomic applications. For example, consider a simple classifier applied to some two-class data such as a scenario with N = 50 samples in two equal-sized classes, and p = 3000 quantitative inputs (standard Normal) that are independent of the class labels. The true (test) error rate of any classifier is 48.9%. Now, we have selected 100 inputs from 3000 inputs having the largest correlation with the class labels over all 50 samples and then used a logistics regression classifier, based on just these 100 inputs. Finally, we use 5-fold cross- validation to estimate the unknown tuning parameters and to estimate the prediction error of the final model. And then over 50 simulations, we found the average cross-validation error rate was 2.9% which is far lower than the true error rate of 48.9%.
Is this a correct application of cross-validation?

ANSWER: No, this is not a correct application of cross-validation.


If not, then what has happened?

ANSWER: it seems like the cross-validation is being applied to select the tuning parameters for the logistic regression model, rather than estimating the performance of the final model.


How do you correctly carry out cross-validation in this example to estimate the test set performance of this classifier?

ANSWER:

To correctly estimate the test set performance of the classifier:

Split the data into training (80%) and test (20%) sets.
Select the top 100 inputs based on correlation with class labels using only the training data.
Train the logistic regression model on the selected features.
Perform 5-fold cross-validation on the training data.
Evaluate the model's performance on the test set using the selected features.
This ensures unbiased estimation of the model's performance on unseen data.


Can you justify these scenarios via a small simulated data experiment?

ANSWER: YES !



# 5

```{r}
# Read the CSV file into a data frame
Prostate_data<- read.csv("prostate.csv")

# Check the structure of the data
print(summary(Prostate_data))
```
```{r}
# Display the first few rows of the data
head(Prostate_data)
```


Visualizing the data: Download the prostate cancer dataset from Moodle and then create a “scatterplot matrix”, i.e. a set of subplots which plots each variable against every other variables,
```{r}
# Load necessary library for plotting
library(ggplot2)

# Create scatterplot matrix
pairs(Prostate_data)

```
## 5b.

Ridge regression:

(i) First, split the data into an outcome vector (y) and a matrix of predictor variables (X) respectively:

load data first
        
y <- prostate[ , 9]
        
X <- prostate[ , - 9]
        
and then set both variables to have zero mean and standardize the predictor variables to have unit variance.


Choose the first 65 patients as the training data. The remaining patients will be the test data.

Write your own code for ridge regression 

Compute the ridge regression solutions for a range of regularizers (lambda). Plot the values of each in the y-axis against (lambda) in the x-axis. This set of plotted values is known as a regularization path. Your plot should look like Figure 1.

```{r}
set.seed(1234)
y <- Prostate_data[, 9]
X <- Prostate_data[, -9]
X <- scale(X)
y <- scale(y, center = TRUE, scale = FALSE)
train_indices <- 1:60
X_train <- X[train_indices, ]
y_train <- y[train_indices]
test_indices <- 61:nrow(X)
X_test <- X[test_indices, ]
y_test <- y[test_indices]
lambda <- 10^seq(-3, 5, length = 50)

ridge <- function(X, y, lambda) {
  X <- cbind(1, X)
  p <- ncol(X)
  n <- nrow(X)
  I <- diag(p)
  theta <- solve(t(X) %*% X + I * lambda) %*% t(X) %*% y
  return(theta)
}

ridge_coefficients <- matrix(data = NA, nrow = ncol(X) + 1, ncol = length(lambda))

for (i in 1:length(lambda)) {
  ridge_coefficients[, i] <- ridge(X_train, y_train, lambda[i])
}

matplot(lambda, t(ridge_coefficients[-1, ]), type = "l", xlab = "Lambda", ylab = "Coefficients", log = "x", main = "Ridge Regression Regularization Path")
legend("topright", legend = colnames(X), col = 1:ncol(X), lty = 1, cex = 0.8)

```
 For each computed value of theta, compute the train and test error. Remember, you will have to standardize your test data with the same means and standard deviations before you can make a prediction and compute your test error since ridge regression assumes the predictors are standardized and the response is centred!
Choose a value of lambda using cross-validation. What is this value? Show all your inter- mediate cross- validation steps and the criterion you used to choose lambda. Plot the train and test errors as a function of lambda. Your plot should look like Figure 2.
```{r}
# Define a function to compute ridge regression
ridge <- function(X_train, y_train, X_valid, y_valid, lambda) {
  X_train <- cbind(1, X_train)  # Add intercept term
  X_valid <- cbind(1, X_valid)  # Add intercept term
  
  # Compute theta using ridge regression formula
  theta <- solve(t(X_train) %*% X_train + lambda * diag(ncol(X_train))) %*% t(X_train) %*% y_train
  
  # Compute predictions
  y_train_pred <- X_train %*% theta
  y_valid_pred <- X_valid %*% theta
  
  # Compute errors
  train_error <- mean((y_train - y_train_pred)^2)
  valid_error <- mean((y_valid - y_valid_pred)^2)
  
  return(list(train_error = train_error, valid_error = valid_error, theta = theta))
}

# Define a function to perform cross-validation and choose lambda
cross_validation <- function(X, y, lambda_values, num_folds) {
  n <- nrow(X)
  fold_indices <- split(1:n, cut(1:n, breaks = num_folds, labels = FALSE))

  train_errors <- matrix(NA, nrow = num_folds, ncol = length(lambda_values))
  valid_errors <- matrix(NA, nrow = num_folds, ncol = length(lambda_values))
  
  # Perform cross-validation
  for (fold in 1:num_folds) {
    train_indices <- unlist(fold_indices[-fold])
    valid_indices <- unlist(fold_indices[fold])
    
    X_train <- X[train_indices, ]
    y_train <- y[train_indices]
    X_valid <- X[valid_indices, ]
    y_valid <- y[valid_indices]
    
    for (i in seq_along(lambda_values)) {
      result <- ridge(X_train, y_train, X_valid, y_valid, lambda_values[i])
      train_errors[fold, i] <- result$train_error
      valid_errors[fold, i] <- result$valid_error
    }
  }
  
  # Average errors across folds
  mean_train_errors <- colMeans(train_errors, na.rm = TRUE)
  mean_valid_errors <- colMeans(valid_errors, na.rm = TRUE)
  
  return(list(mean_train_errors = mean_train_errors, mean_valid_errors = mean_valid_errors))
}

# Define lambda values (logarithmic scale)
lambda_values <- 10^seq(-3, 3, length.out = 100)

# Perform cross-validation
cv_result <- cross_validation(X_train, y_train, lambda_values, num_folds = 5)

# Choose lambda with minimum validation error
best_lambda <- lambda_values[which.min(cv_result$mean_valid_errors)]

# Train the final model using the chosen lambda
final_model <- ridge(X_train, y_train, X_train, y_train, best_lambda)

# Compute test error
final_test_error <- ridge(X_train, y_train, X_test, y_test, best_lambda)$valid_error

# Plot train and validation errors as a function of lambda
plot(lambda_values, cv_result$mean_train_errors, type = "l", col = "blue", xlab = "Lambda", ylab = "Error", main = "Train and Validation Errors vs. Lambda (log scale)", log = "x")
lines(lambda_values, cv_result$mean_valid_errors, type = "l", col = "red")
legend("topright", legend = c("Train Error", "Validation Error"), col = c("blue", "red"), lty = 1)
abline(v = best_lambda, lty = 2, col = "green")
text(best_lambda, max(cv_result$mean_valid_errors), paste("Lambda =", round(best_lambda, 4)), pos = 4, col = "green")

```
For the best theta, plot separately (using subplots) the train and test error as a function of the patient number. That is, for each patient show the actual response and the prediction.
```{r}


```

## 5c.

Lasso regression: We will now implement the Lasso and try this code out on the prostate cancer data.
We know that the most popular approach for fitting lasso and other penalized regres- sion models is to employ coordinate descent algorithms (aka “shooting” method), a less beautiful but simpler and more flexible alternative. The idea behind coordinate descent is, simply, to optimize a target function with respect to a single parameter at a time, iteratively cycling through all parameters until convergence is reached.

(i) Implement the coordinate descent for solving Lasso. The coordinate descent algo- rithm is implemented in the R package glmnet. You can use glmnet or caret package in R to solve this part. You should look at “Unit 7: Regularization” lecture slides (data application part) for a better understanding.
```{r}
# Install and load the glmnet package
library(glmnet)

# Assuming you have your data ready: X_train and y_train for training

# Fit Lasso regression model using glmnet
lasso_model <- glmnet(X_train, y_train, alpha = 1)  # Set alpha = 1 for Lasso

# Optionally, perform cross-validation to select lambda
cv_model <- cv.glmnet(X_train, y_train, alpha = 1)

# Get the optimal lambda value
optimal_lambda <- cv_model$lambda.min  # or cv_model$lambda.1se for a less complex model

# Refit the model with the optimal lambda
lasso_model_optimal <- glmnet(X_train, y_train, alpha = 1, lambda = optimal_lambda)

# Extract coefficients
lasso_coefficients <- coef(lasso_model_optimal)

# Make predictions
# Assuming you have your test data ready: X_test for testing
y_pred <- predict(lasso_model_optimal, newx = X_test)

# Evaluate model performance
# Assuming y_test contains the true labels for the test data
mse <- mean((y_test - y_pred)^2)

```

 Find the solutions and generate the plots from (iii – v) of the previous question, but now using this new Lasso estimate.
```{r}
# Load required library
library(glmnet)

# Convert data to matrix format
X_train_matrix <- as.matrix(X_train)
X_test_matrix <- as.matrix(X_test)

# Fit Lasso model
lasso_model <- glmnet(X_train_matrix, y_train, alpha = 1, lambda = lambda_values)

# Extract coefficients
lasso_coefficients <- coef(lasso_model)

# Plot coefficients as a function of lambda
plot(lasso_model, xvar = "lambda", label = TRUE)

```

```{r}
# Predictions on test set
lasso_test_pred <- predict(lasso_model, newx = X_test_matrix, s = best_lambda)

# Compute test error
lasso_test_error <- mean((y_test - lasso_test_pred)^2)
print(lasso_test_error)
```
Compare the results obtained from Ridge and Lasso regression. What do you learn from the analysis of the prostate cancer data?

ANSWER:

Comparing the results obtained from Ridge and Lasso regression on the prostate cancer data can provide valuable insights into the effectiveness of each method:


# Question 4

This question involves the use of Bootstrap on simulated data.

Suppose that we wish to invest a fixed sum of money in two financial assets (say, Apple, IBM) that yield returns of X and Y, respectively, where X and Y are random quantities. We will invest a fraction alpha of our money in X, and will invest the remaining (1-alpha) in Y. We wish to choose alpha to minimize the total risk, or variance, of our investment. In other words, we want to minimize. 

Perform bootstrap on this example to see the variability of the sample estimator alpha over 1000 simulations (data sets) from the true population and to estimate the standard deviation of alpha. Also calculate bootstrap bias estimate and a basic bootstrap confidence interval for alpha. Please ensure that the results are reproducible (i.e, setting a seed in R).

ANSWER: 

To perform the bootstrap analysis described in the question, we need to follow these steps:

Generate simulated returns for investments X and Y.
Estimate the true value of alpha using the formula provided.
Write a function to estimate alpha using the provided equation.
Draw 1000 bootstrap samples from the true population with replacement.
Calculate an estimate of alpha from each bootstrap sample.
cmpute the standard deviation of true alpha, bootstrap buias estimate, and a basic bootstrap confidence interval for alpha
 .

```{r}
# Set seed for reproducibility
set.seed(123)

# Function to generate simulated returns for investments X and Y
generate_returns <- function(n, rho = 0.4) {
  Z1 <- rnorm(n)
  Z2 <- rnorm(n)
  X <- Z1
  Y <- rho * Z1 + sqrt(1 - rho^2) * Z2
  return(list(X = X, Y = Y))
}

# Function to estimate alpha using the provided equation
estimate_alpha <- function(X, Y) {
  sigma_X2 <- var(X)
  sigma_Y2 <- var(Y)
  sigma_XY <- cov(X, Y)
  alpha <- (sigma_Y2 - sigma_XY) / (sigma_X2 + sigma_Y2 - 2 * sigma_XY)
  return(alpha)
}

# True estimate of alpha using simulated returns
simulated_returns <- generate_returns(100)
true_alpha <- estimate_alpha(simulated_returns$X, simulated_returns$Y)

# Bootstrap analysis
num_simulations <- 1000
bootstrap_alphas <- numeric(num_simulations)
for (i in 1:num_simulations) {
  # Generate bootstrap sample
  bootstrap_sample <- sample(1:length(simulated_returns$X), replace = TRUE)
  bootstrap_X <- simulated_returns$X[bootstrap_sample]
  bootstrap_Y <- simulated_returns$Y[bootstrap_sample]
  
  # Estimate alpha for the bootstrap sample
  bootstrap_alpha <- estimate_alpha(bootstrap_X, bootstrap_Y)
  bootstrap_alphas[i] <- bootstrap_alpha
}

# Standard deviation of alphâ
alpha_sd <- sd(bootstrap_alphas)

# Bootstrap bias estimate
bootstrap_bias <- mean(bootstrap_alphas) - true_alpha

# Basic bootstrap confidence interval for alpha
alpha_ci <- quantile(bootstrap_alphas, c(0.025, 0.975))

# Print results
cat("True Estimate of Alpha:", true_alpha, "\n")
cat("Standard Deviation of Alphâ:", alpha_sd, "\n")
cat("Bootstrap Bias Estimate:", bootstrap_bias, "\n")
cat("Bootstrap Confidence Interval for Alpha:", alpha_ci, "\n")

```
# Question 2

This question involves the use of K-Nearest Neighbour (KNN) on the red wine quality data set from the UCI repository. Use R to complete these tasks. Make sure you included all the R codes.

## 2a
Write the goal of this data analysis. List the inputs and output.Do some exploratory data analysis (EDA) first. Process any necessary data transformation. Explain why you are using that transformation. This could include:
• Feature scaling such as standardizing or normalizing the data.
• Selecting or removing certain values (such as outliers or missing values).



ANSWER: 

The goal of this data analysis is to explore and understand the relationship between different attributes or features of red wine and its quality. This can involve identifying important factors that contribute to the quality of red wine and potentially building a predictive model to estimate wine quality based on these attributes.

Inputs:

Various chemical properties or attributes of red wine, such as fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol content.
Output:

The quality of red wine, which is typically represented as an ordinal or categorical variable. This output variable is often scored based on sensory evaluations or expert judgments, ranging from low to high quality.
```{r}
# Load necessary libraries
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)

# Read the data
red_wine_data <- read_csv("winequality-red.csv", col_names = FALSE)

# View the first few rows of the data
head(red_wine_data)

```

```{r}

# Read the data with appropriate delimiter and skip the first row
red_wine_data <- read_csv("winequality-red.csv", skip = 1, col_names = FALSE)

# Separate the columns based on the delimiter ";"
red_wine_data <- separate(red_wine_data, col = 1, into = c("fixed_acidity", "volatile_acidity", "citric_acid", 
                                                           "residual_sugar", "chlorides", "free_sulfur_dioxide",
                                                           "total_sulfur_dioxide", "density", "pH", "sulphates",
                                                           "alcohol", "quality"), sep = ";")

# Convert columns to numeric
red_wine_data <- mutate_all(red_wine_data, as.numeric)

# View the structure of the dataset
str(red_wine_data)

# Summary statistics
summary(red_wine_data)


# Exploratory data analysis (EDA)
# Visualize the distribution of quality scores
ggplot(red_wine_data, aes(x = quality)) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Distribution of Wine Quality",
       x = "Quality Score",
       y = "Frequency")

# Explore the relationship between quality and other variables
# For example, let's visualize the relationship between alcohol content and quality
ggplot(red_wine_data, aes(x = quality, y = alcohol)) +
  geom_boxplot(fill = "lightgreen", color = "black") +
  labs(title = "Relationship between Alcohol Content and Wine Quality",
       x = "Quality Score",
       y = "Alcohol Content")

```
Separating the Columns: The data read from the CSV file contains all the columns in a single column due to the semicolon delimiter. We use the separate function from the tidyr package to split this single column into multiple columns based on the semicolon delimiter. The into argument specifies the names of the resulting columns.

Converting Columns to Numeric: Initially, all columns are read as character data types. However, we want to perform numerical analysis on the data, so we convert all columns to numeric using the mutate_all function from the dplyr package. This ensures that the data in each column is treated as numeric rather than character.

Exploratory Data Analysis (EDA): After cleaning and preparing the data, we perform some exploratory data analysis (EDA) to gain insights into the data. Specifically, we visualize the distribution of wine quality scores using a bar plot and explore the relationship between wine quality and alcohol content using a box plot

#### AFTER CLEANING RESULT
```{r}
# View the first few rows of the cleaned and prepared data
head(red_wine_data)

```


```{r}
# Check for missing values
sum(is.na(red_wine_data))
```
```{r}
# Impute missing values with mean
red_wine_data <- na.omit(red_wine_data)  # Remove rows with missing values

# Check for missing values after imputation
sum(is.na(red_wine_data))

```
imputed missing values with the mean to handle the presence of missing data in the dataset. By performing mean imputation and removing rows with missing values, you ensure that the dataset is complete and ready for further analysis without losing a substantial amount of data.

```{r}
# Compute correlation matrix
corr_matrix <- cor(red_wine_data)

# Plot correlation matrix
library(corrplot)
corrplot(corr_matrix, method = "color", type = "upper", tl.cex = 0.7, diag = FALSE)

```
```{r}
# Standardize the features
standardize <- function(x) {
  return((x - mean(x)) / sd(x))
}

red_wine_data_scaled <- red_wine_data %>%
  mutate(across(where(is.numeric), standardize))

# View the first few rows of the scaled dataset
head(red_wine_data_scaled)
```
Standardizing the features (also known as z-score normalization) is a common preprocessing step in machine learning to ensure that all features have the same scale. This is important because features with larger scales may dominate the learning process, leading to biased models.

## 2b

Build a KNN (K-Nearest Neighbour) classifier to predict wine quality using red wine quality data set. To get a better result, you may need to think to reduce the categories of the outcome.
```{r}
# Load necessary libraries
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(class)  # Load the class package for knn function

# Read the data with appropriate delimiter and skip the first row
red_wine_data <- read_csv("winequality-red.csv", skip = 1, col_names = FALSE)

# Separate the columns based on the delimiter ";"
red_wine_data <- separate(red_wine_data, col = 1, into = c("fixed_acidity", "volatile_acidity", "citric_acid", 
                                                           "residual_sugar", "chlorides", "free_sulfur_dioxide",
                                                           "total_sulfur_dioxide", "density", "pH", "sulphates",
                                                           "alcohol", "quality"), sep = ";")

# Convert columns to numeric
red_wine_data <- mutate_all(red_wine_data, as.numeric)

# Create binary response variable
red_wine_data$binary_response <- ifelse(red_wine_data$quality > 5, "high", "low")

# Split data into training and testing sets
set.seed(730216)
data_split <- createDataPartition(red_wine_data$quality, p = 0.8, list = FALSE)
train_wine <- red_wine_data[data_split, ]
test_wine <- red_wine_data[-data_split, ]

# Define k value
k <- 5

# Build the KNN model
knn_model <- knn(train = train_wine[, -c(1, 13)], test = test_wine[, -c(1, 13)],
                 cl = train_wine$binary_response, k = k)

# Compute the confusion matrix
confusion_matrix <- confusionMatrix(knn_model, as.factor(test_wine$binary_response))

# Calculate accuracy
accuracy <- confusion_matrix$overall["Accuracy"]
print(accuracy)

# Print confusion matrix
print(confusion_matrix)

```
This indicates that the model performs relatively well in predicting wine quality based on the features provided.

## 2C

Apply cross-validation. Which kind of cross-validation do you think is appropriate? Find the optimal value of K? You can use the train function under caret package in R for this.
```{r}
# Load necessary libraries
library(caret)
library(class)

# Define the training control
train_control <- trainControl(method = "cv",   # Use k-fold cross-validation
                              number = 10)     # Specify the number of folds (e.g., 10)

# Define the grid of K values to search over
k_values <- seq(1, 20, by = 1)  # Example: Search K values from 1 to 20

# Train the KNN model using cross-validation
knn_model_cv <- train(binary_response ~ .,                   # Formula for the model
                       data = red_wine_data,                 # Data to train on
                       method = "knn",                       # KNN method
                       trControl = train_control,            # Training control settings
                       tuneGrid = data.frame(k = k_values))  # Grid of K values to search over

# View the results
print(knn_model_cv)

# Plot the results
plot(knn_model_cv)

```
## 2d.

Print out your algorithm performance. Choose the right metric(s) for judging the ef- fectiveness of your prediction. You should evaluate the model performance using the Confusion Matrix.
```{r}
# Predictions using the trained model
predictions <- predict(knn_model_cv, newdata = red_wine_data)

# Create the confusion matrix
confusion_matrix <- table(predictions, red_wine_data$binary_response)

# Print the confusion matrix
print(confusion_matrix)

# Calculate additional performance metrics
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])  # Positive predictive value
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])     # True positive rate
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the performance metrics
cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")

```
 the perfect performance metrics suggest that the dataset may have been relatively simple or that the model may have overfit the data. It's crucial to further investigate and validate the model's performance on unseen data to ensure its reliability and generalizability.

I learn that the features included in the dataset are highly informative and can effectively discriminate between different quality levels of wine. The KNN algorithm, when applied to this dataset, was able to leverage these features to make accurate predictions.

